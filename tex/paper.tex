\documentclass[sigconf]{acmart}

\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For formal tables
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{todonotes}
\usepackage{svg}

\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} (\nameref*{#1})}} % One single link

% Adds a "given" symbol (vertical bar) that rescales in height (like \left and \right)
\usepackage{mathtools}
\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase
\newcommand\sgiven{\givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\given\sgiven #1}
\newcommand\Average{E\Basics}

\makeatletter
\newcommand{\StateIndent}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \Statex\hskip\dimexpr#1\@tempdima\relax}
\algdef{S}[WHILE]{WhileNoDo}[1]{\algorithmicwhile\ #1}%
\makeatother

\newcommand{\algorithmautorefname}{Algorithm}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA} 
\acmYear{2017}
\copyrightyear{2017}

\acmPrice{15.00}


\begin{document}
\title{Let's Play Congestion Control}
\titlenote{Produces the permission block, and
  copyright information}
\subtitle{Extended Abstract}
\subtitlenote{The full version of the author's guide is available as
  \texttt{acmart.pdf} document}

\author{Maximilian Bachl} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{maximilian.bachl@tuwien.ac.at}

\author{Tanja Zseby} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{tanja.zseby@tuwien.ac.at}

\author{Joachim Fabini} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{joachim.fabini@tuwien.ac.at}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Bachl, Zseby, Fabini}

\begin{abstract}
This paper provides a sample of a \LaTeX\ document which conforms,
somewhat loosely, to the formatting guidelines for
ACM SIG Proceedings.\footnote{This is an abstract footnote}
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Put some comma-separated keywords here}


\maketitle

\section{Partial Action Actor Critic Learning}
\label{sec:pal}

Partial Action Actor Critic Learning (PAL) is a modification of the Asynchronous Advantage Actor Critic algorithm proposed by \citet{mnih_asynchronous_2016}. It consists of two Artificial Neural Networks (ANNs), the Actor Network and the Value Network (the Critic part in the abbreviation stands for the Value Network). Given a state, the Actor Networks outputs what it deems to be the optimum action in that certain state. The Value Network estimates what long-term reward can be expected in this state. So an action is considered good if it achieved a long-term reward that is higher than the long-term reward expected by the Value Network and it is considered bad if the reward was lower than expected. The long-term reward is implemented as a moving average of future rewards. So if a high reward can be achieved right now this is more favorable than if it can be achieved in the future. However it can also be beneficial to get a low reward now and instead get a very large one in the future. 

\subsection{Algorithm}
\label{subsec:alg}

In \autoref{alg:pal} we show the code that runs in one of the learning threads. As proposed in the literature, we use several concurrent thread to improve training performance. A major difference to previous approaches to reinforcement learning is the fact that we allow more actions to be taken before the previous actions' rewards have been received. In classical reinforcement learning, an action is always followed by a reward and a reward is always followed by a action. In our proposed concept, however, it is possible to take new actions while the previous action hasn't received its reward yet.

\begin{figure}
\begin{minipage}{\columnwidth}
\includesvg{figures/Reinforcement_learning_diagram}
\caption{The classical reinforcement learning approach.\protect\footnote{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}}
\label{fig:reinforcement}
\end{minipage}
\end{figure}

Another major difference in PAL is that one action generates a number of partial actions ($\leq 1$) (see \autoref{fig:pal}). Each partial interaction generates a partial reward upon generating with the environment. Upon receiving a partial reward, the agent estimates the current state and triggers a new action. When all partial rewards of one action were received, the agent combines them to form the reward and updates the value and actor networks (see \autoref{alg:grad}). 

\begin{figure}
\begin{minipage}{\columnwidth}
\includesvg{figures/Reinforcement_learning_diagram_PAL}
\caption{Partial Action Actor Critic Learning: An action consists of zero or more partial actions which trigger partial rewards upon interacting with the environment. Each partial reward updates the state. The value and actor networks are updated upon receiving all partial rewards of one action.\protect\footnote{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}}
\label{fig:pal}
\end{minipage}
\end{figure}

\begin{algorithm}
\caption{Partial Action Actor Critic Learning -- pseudocode for each learning thread. Only $\theta_{\text{a},\text{g}}$ and $\theta_{\text{v},\text{g}}$ are shared between the threads. They are initialized randomly in the beginning.}
\label{alg:pal}
\begin{algorithmic}[1]
\Loop
	\State $l_\text{actions} \gets  [] $
	\State $l_\text{states} \gets  [] $
	\State $l_\text{values} \gets  [] $
	\State $l_\text{rewards} \gets  [] $
	\State $l_\text{estimatedValues} \gets  [] $
	\State $l_\text{snapshots} \gets  [] $
	\State $t \gets 0$
	\State $s_0 \gets \text{initialState}()$
	\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
	\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
	\Repeat
		\State $l_\text{states}.\text{append}(s_t)$
		\State $l_\text{estimatedValues}.\text{append}(V(s_t; \theta_\text{v}))$
		\If{$t \bmod t_\text{max} = 0$}
			\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
			\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
			\State $l_\text{snapshots}.\text{append}((\theta_\text{a}, \theta_\text{v}))$
		\EndIf
		\State Sample $a_t$ from the 
		\StateIndent[3] Actor Network's probability distribution
		\State $l_\text{actions}.\text{append}(a_t)$
		\State $l_{\text{partialActions},t} \gets \text{partialActions}(a_t)$
		\ForAll{partial actions $a_{{\text{p},n,t}}$ in $l_{\text{partialActions},t}$}
			\State Take partial action $a_{{\text{p},n,t}}$
		\EndFor
		\State Receive partial reward $r_{\text{p},n,t'}$
		\StateIndent[3] where $t' \leq t$ and $0 \leq n \leq\#(l_{\text{partialActions},t'})$
		\If{all partial rewards of $a_{t'}$ were received}
			\State $r_{t'} \gets \text{combination of all partial rewards }r_{\text{p},n,t'}$
			\State $l_\text{rewards}.\text{append}(r_{t'})$
			\If{$\#\left(l_\text{rewards}\right) > t_\text{max}$ or the episode is over}
				\State \Call{computeGradients}{}
			\EndIf
		\EndIf
		\State Generate $s_{t+1}$ using $r_{\text{p}_{t'}}$
		\State $t \gets t+1$
	\Until{reaching the end the episode}
\EndLoop
%\algstore{palalg}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Partial Action Actor Critic Learning -- procedure which computes and applies the gradients.}
\label{alg:grad}
\begin{algorithmic}[1]%
%\algrestore{palalg}
\Function{computeGradients}{}
	\State $t_\text{end} = \min\left(t_\text{max}, \#\left( l_\text{rewards} \right)\right)$
	\State $\theta_\text{backup} \gets \left(\theta_\text{a}, \theta_\text{v}\right)$
	\State $\theta_\text{a}, \theta_\text{v} \gets l_\text{snapshots}[0]$
	\State $R_{i+1} \gets \frac{l_\text{estimatedValues}[t_\text{end}]}{1-\gamma}$
	\For{$i\gets t_\text{end}-1, 0$}
		\State $R_i \gets r_i + \gamma R_{i+1}$
		\State $a \gets l_\text{actions}[i]$
		\State $s \gets l_\text{states}[i]$
		\State $v \gets l_\text{values}[i]$
		\State $d\theta_\text{v} \gets \frac{\partial\left(R_i - v\right)}{\partial\theta_v}$
		\State $d\theta_\text{v} \gets -\frac{\partial\log \left( \pi_W\left( a-1 < W \leq a \given s; \theta_\text{a} \right)\right)\left(R_i - v\right)}{\partial\theta_a}$
		\StateIndent[4] $-\frac{\partial\beta H\left( \pi\left( s; \theta_a \right)\right)}{\partial\theta_a}$
		\State $\theta_{\text{a},\text{g}} \gets \theta_{\text{a},\text{g}} + d\theta_\text{a}$
		\State $\theta_{\text{v},\text{g}} \gets \theta_{\text{v},\text{g}} + d\theta_\text{v}$
	\EndFor
	\State Remove first $t_\text{end}$ elements from
	\StateIndent[2] $l_\text{actions}$, $l_\text{states}$, $l_\text{values}$, $l_\text{rewards}$, $l_\text{estimatedValues}$
	\State Remove first element from $l_\text{snapshots}$
	\State $\theta_\text{a}, \theta_\text{v} = \theta_\text{backup}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Value Network}
\label{subsec:genericvalue}

The value network estimates the expected long-term reward given a state $s_t$.

Let $r_t$ be the reward that was received at time $t$. $V(s_t; \theta_\text{v})$ designates the expected mean reward in state $s_t$ given the parameters of the value network $\theta_v$. Then, with $\gamma$ being the roll-off factor, which designates the influence that future reward has on the moving average, (set to $0.99$), we define the average reward as 
\begin{align*}
R_t = \left(\left(\sum_{i=0}^{k-1} \gamma^ir_{t+i}\right) + \gamma^k \frac{V(s_{t+k}; \theta_\text{v})}{1-\gamma}\right)\left(1-\gamma\right),
\end{align*}
where $k$ is upper-bounded by $t_\text{max}$ ($t_\text{max}$ is a fixed hyperparameter that indicates how many steps should be performed before updating the neural network). So $R$ is simply a moving average of rewards. However, usual moving averages take into account values from the past while this one uses values from the future; it runs reversely. 

The loss function, which the value network tries to minimize, is the square of the difference of the actual average reward received and the expected average reward
\begin{align*}
l_{\text{v}_t} = \left(R_t - V(s_t; \theta_\text{v})\right)^2.
\end{align*}

\subsection{Actor Network}
\label{subsec:genericactor}

Let $a_t$ be the action that was taken at time step $t$, $v_t$ the value that the value network estimated as the future reward given the current state $s_t$ at time step $t$ ($v_t$ is just an abbreviation for $V(s_t; \theta_\text{v})$) and $\theta_a$ the parameters of the actor network. Furthermore, let $\beta$ be a factor that specifies the importance of the entropy $H$. $\pi$ designates the probability density function, so $\pi\left( a_t \given s_t; \theta_\text{a} \right)$ is the value of the probability density function of taking action $a_t$ in state $s_t$ with the current weights of the actor network $\theta_\text{a}$. Then
\begin{align*}
l_{\text{a}_t} =& -\log \left( \pi\left( a_t \given s_t; \theta_\text{a} \right)\right)\left( R_t - v_t \right)\\ 
&- \beta H\left( \pi\left( s_t; \theta_a \right)\right)
\end{align*}
is the actor loss that we try to minimize. 

\section{Congestion Control specifics}

The classical Reinforcement Learning assumes that a reward follows an action and vice-versa (see \autoref{fig:reinforcement}). However, in the case of congestion control, it is desirable to perform a new action without having received a reward for the previous action. For example, imagine that you receive two acknowledgements directly after each other. For each of these two acknowledgements an action has to be performed but it does not seem feasible for the second action to wait until the first action has received a reward. The Asynchronous Actor Critic framework as described by \cite{mnih_asynchronous_2016} cannot be applied to congestion control as it assumes that actions and rewards are synchronized and thus we use the proposed Partial Action Actor Critic Learning (see \autoref{sec:pal})

In the following a time step $t$ corresponds to the reception of an acknowledgement. The beginning of the flow, before any packet is sent, corresponds to time step $0$.

\begin{description}
\item[$\textit{s}_\text{t}$] The state  describes the current ``congestion state''. Various features that indicate congestion are included in it. Each time an acknowledgement is received, the state is updated and the actor network is asked for the next action. 
\item[$\textit{a}_\text{t}$] Based on a given state and the history of previous states, the actor network returns an action $a_t$, which is an integer ($\geq 1$) that stands for the congestion window to be used until the next acknowledgement is received. 
\item[$\textit{r}_\text{t}$] The reward is a tuple of at least one reward metrics. These are discussed in more detail in \autoref{subsec:value}
\item[$\textit{v}_\text{t}$] The value is a tuple of the average expected rewards estimated by the value network (see \autoref{subsec:value}) for each of the reward metrics. 
\end{description}

We actually use four different types of reward: 
\begin{description}
\item[$\textit{r}_{\text{packet},t}$] is the number of the packets that the sender sent during time step $t$ and that were not lost (so they were acknowledged at some point by the receiver).
\item[$\textit{r}_{\text{byte},t}$] is the sum of the bytes of the packets that the sender sent and that were not lost. Example: During $t$, three packets were sent with 300, 200, and 1500 bytes each so $\textit{r}_{\text{byte},t} = 2000$
\item[$\textit{r}_{\text{delay},t}$] is the sum of the round trip times of the packets that the sender sent and that were not lost.
\item[$\textit{r}_{\text{duration},t}$] is the sum of the time between receiving the last packet and receiving this packet (``inter-receive time'') for the packets that the sender sent and that were not lost.
\end{description}

%We actually use four moving averages for four different types of reward: 
%\begin{description}
%\item[$\textit{R}_\text{packet}$] is the moving average of the packets that the sender sent and that were not lost (so they were acknowledged at some point by the receiver).
%\item[$\textit{R}_\text{byte}$] is the moving average of the bytes of the packets that the sender sent and that were not lost.
%\item[$\textit{R}_\text{delay}$] is the moving average of the round trip time of the packets that the sender sent and that were not lost.
%\item[$\textit{R}_\text{duration}$] is the moving average of the time between receiving the last packet and receiving this packet (``inter-receive time'') for the packets that the sender sent and that were not lost.
%\end{description}

\subsection{Value Network}
\label{subsec:value}

In the case of congestion control, the loss function $l_{v_t}$ of the value network is actually the sum of the square of the difference for each of the four moving averages for each type of reward:
\begin{align*}
l_{\text{v}_t} =& \left(R_{\text{packet},t} - V_\text{packet}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{byte},t} - V_\text{byte}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{delay},t} - V_\text{delay}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{duration},t} - V_\text{duration}(s_t; \theta_\text{v})\right)^2
\end{align*}

\subsection{Actor Network}
\label{subsec:actor}

The actor network outputs two parameters: The mean of a log-normal distribution $\mu_\text{log}$ and its standard deviation $\sigma_\text{log}$. 

We calculate the means and standard deviation of the underlying normal distribution as
\begin{align*}
\sigma_\text{normal} &= \sqrt{\log \left( \frac{\sigma_\text{log}}{\mu_\text{log}} +1 \right)} \\
\mu_\text{normal} &= \log\left(\mu_\text{log}\right) - \frac{{\sigma_\text{normal}}^2}{2}.
\end{align*}

Each time an action $a_t$ is requested, a value $X$ is sampled from the current log-normal distribution defined by the parameters $\mu_\text{normal}$ and $\sigma_\text{normal}$: 
\begin{align*}
Z&\sim {\mathcal {N}}(\mu_\text{normal} ,{\sigma_{\text{normal}}}^{2})\\
X&=\exp(Z)
\end{align*}

We can also equally write
\begin{align*}
X \sim \text{Lognormal}\left( \mu_\text{normal}, {\sigma_{\text{normal}}}^{2}\right).
\end{align*}

The sampled value $X$ is then discretized by rounding up, which also prevents window sizes smaller than one from occurring:
\begin{align*}
W = \left\lceil X \right\rceil
\end{align*}

$a_t$ is the sampled value $W$ at time $t$ and it is used as the window until the next time step $t+1$ begins (at the reception of the next acknowledgement). 

%Let
%\begin{align*}
%\Sigma R_t = \sum_{i \in \left \{\text{packet}, \text{byte}, \text{delay}, \text{duration}\right \}} R_{i_t}
%\end{align*}
%and
%\begin{align*}
%\Sigma v_t = \sum_{i \in \left \{\text{packet}, \text{byte}, \text{delay}, \text{duration}\right \}} v_{i_t}.
%\end{align*}

With $H$ being the entropy, the actor network minimizes the loss
\begin{align*}
l_{\text{a}_t} =& -\log \left( \pi_W\left( a_t-1 < W \leq a_t \given s_t; \theta_\text{a} \right)\right)\\
&\left( \left(\frac{R_{\text{byte},t}}{{R_{\text{duration},t}}} - \frac{v_{\text{byte},t}}{{v_{\text{duration},t}}}\right) - \left( \frac{R_{\text{delay},t}}{{R_{\text{packet},t}}}- \frac{v_{\text{delay},t}}{{v_{\text{packet},t}}} \right)\right)\\ 
&+ \beta H\left( \pi\left( s_t; \theta_a \right)\right).
\end{align*}
 
\bibliographystyle{ACM-Reference-Format}
\bibliography{congestion}

\end{document}
