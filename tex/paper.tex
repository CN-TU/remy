\documentclass[sigconf]{acmart}

\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For formal tables
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{todonotes}
\usepackage{svg}

\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} (\nameref*{#1})}} % One single link

% Adds a "given" symbol (vertical bar) that rescales in height (like \left and \right)
\usepackage{mathtools}
\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase
\newcommand\sgiven{\givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\given\sgiven #1}
\newcommand\Average{E\Basics}

\makeatletter
\newcommand{\StateIndent}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \Statex\hskip\dimexpr#1\@tempdima\relax}
\algdef{S}[WHILE]{WhileNoDo}[1]{\algorithmicwhile\ #1}%
\makeatother

\newcommand{\algorithmautorefname}{Algorithm}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA} 
\acmYear{2017}
\copyrightyear{2017}

\acmPrice{15.00}


\begin{document}
\title{Let's Play Congestion Control}
\titlenote{Produces the permission block, and
  copyright information}
\subtitle{Extended Abstract}
\subtitlenote{The full version of the author's guide is available as
  \texttt{acmart.pdf} document}

\author{Maximilian Bachl} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{maximilian.bachl@tuwien.ac.at}

%\author{Joachim Fabini} 
%\affiliation{%
% \institution{Institute of Telecommunications}
% \streetaddress{Gußhausstraße 25}
% \postcode{1040}
% \city{Vienna} 
% \country{Austria}}
%\email{joachim.fabini@tuwien.ac.at}

\author{Tanja Zseby} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{tanja.zseby@tuwien.ac.at}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Bachl, Zseby}


\begin{abstract}
This paper provides a sample of a \LaTeX\ document which conforms,
somewhat loosely, to the formatting guidelines for
ACM SIG Proceedings.\footnote{This is an abstract footnote}
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Put some comma-separated keywords here}


\maketitle

\section{Introduction}

In the following a time step $t$ corresponds to the reception of an acknowledgement. The beginning of the flow, before any packet is sent, corresponds to time step $0$.

\begin{description}
\item[$\textit{s}_\text{t}$] The state  describes the current ``congestion state''. Various features that indicate congestion are included in it. Each time an acknowledgement is received, the state is updated and the actor network is asked for the next action. 
\item[$\textit{a}_\text{t}$] Based on a given state and the history of previous states, the actor network returns an action $a_t$, which is an integer ($\geq 1$) that stands for the congestion window to be used until the next acknowledgement is received. 
\item[$\textit{r}_\text{t}$] The reward is a tuple of at least one reward metrics. These are discussed in more detail in \fullref{subsec:value}
\item[$\textit{v}_\text{t}$] The value is a tuple of the rewards estimated by the value network (see \fullref{subsec:value}) for each of the reward metrics. 
\end{description}

\subsection{Value Network}
\label{subsec:value}

The value network estimates the expected reward given a state $s_t$.

Let $r_t$ be the reward that was received at time $t$. $V(s_t; \theta_\text{v})$ designates the expected mean reward in state $s_t$ given the parameters of the value network $theta_v$. Then, with $\gamma$ being the roll-off factor, which designates the influence that future reward has on the moving average, (set to $0.99$), we generically define the expected n-step return as 
\begin{align*}
R_t = \left(\left(\sum_{i=0}^{k-1} \gamma^ir_{t+i}\right) + \gamma^k \frac{V(s_{t+k}; \theta_\text{v})}{1-\gamma}\right)\left(1-\gamma\right),
\end{align*}
where $k$ is upper-bounded by $t_\text{max}$. $R$ is simply a moving average of future rewards. However, usual moving averages take into account values from the past while this one uses values from the future; it runs reversely. 

The loss function, which the value network tries to minimize, is the square of the difference of the actual mean reward received and the expected mean reward
\begin{align*}
l_{\text{v}_t} = \left(R_t - V(s_t; \theta_\text{v})\right)^2.
\end{align*}

We actually use four moving averages for four different types of reward: 
\begin{description}
\item[$\textit{R}_\text{packet}$] is the moving average of the packets that the sender sent and that were not lost (so they were acknowledged at some point by the receiver)
\item[$\textit{R}_\text{byte}$] is the moving average of the bytes of the packets that the sender sent and that were not lost
\item[$\textit{R}_\text{delay}$] is the moving average of the round trip time of the packets that the sender sent and that were not lost
\item[$\textit{R}_\text{duration}$] is the moving average of the time between receiving the last packet and receiving this packet (``inter-receive time'') for the packets that the sender sent and that were not lost
\end{description}

The loss function $l_{v_t}$ of the value network is actually the sum of the square of the difference for each of these four moving averages:
\begin{align*}
l_{\text{v}_t} =& \left(R_{\text{packet},t} - V_\text{packet}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{byte},t} - V_\text{byte}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{delay},t} - V_\text{delay}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{duration},t} - V_\text{duration}(s_t; \theta_\text{v})\right)^2
\end{align*}

\subsection{Actor Network}
\label{subsec:actor}

The actor network outputs two parameters: The mean of a log-normal distribution $\mu_\text{log}$ and its standard deviation $\sigma_\text{log}$. 

We calculate the means and standard deviation of the underlying normal distribution as
\begin{align*}
\sigma_\text{normal} &= \sqrt{\log \left( \frac{\sigma_\text{log}}{\mu_\text{log}} +1 \right)} \\
\mu_\text{normal} &= \log\left(\mu_\text{log}\right) - \frac{{\sigma_\text{normal}}^2}{2}.
\end{align*}

Each time an action $a_t$ is requested, a value $X$ is sampled from the current log-normal distribution defined by the parameters $\mu_\text{normal}$ and $\sigma_\text{normal}$: 
\begin{align*}
Z&\sim {\mathcal {N}}(\mu_\text{normal} ,{\sigma_{\text{normal}}}^{2})\\
X&=\exp(Z)
\end{align*}

We can also equally write
\begin{align*}
X \sim \text{Lognormal}\left( \mu_\text{normal}, {\sigma_{\text{normal}}}^{2}\right).
\end{align*}

The sampled value $X$ is then discretized by rounding up, which also prevents window sizes smaller than one from occurring:
\begin{align*}
W = \left\lceil X \right\rceil
\end{align*}

$a_t$ is the sampled value $W$ at time $t$ at it is used as the window until the $t+1$, which is the reception of the next acknowledgement. 

%Let
%\begin{align*}
%\Sigma R_t = \sum_{i \in \left \{\text{packet}, \text{byte}, \text{delay}, \text{duration}\right \}} R_{i_t}
%\end{align*}
%and
%\begin{align*}
%\Sigma v_t = \sum_{i \in \left \{\text{packet}, \text{byte}, \text{delay}, \text{duration}\right \}} v_{i_t}.
%\end{align*}

With $H$ being the entropy, the actor network minimizes the loss
\begin{align*}
l_{\text{a}_t} =& -\log \left( \pi_W\left( a_t-1 < W \leq a_t \given s_t; \theta_\text{a} \right)\right)\\
&\left( \left(\frac{R_{\text{byte},t}}{{R_{\text{duration},t}}} - \frac{v_{\text{byte},t}}{{v_{\text{duration},t}}}\right) - \left( \frac{R_{\text{delay},t}}{{R_{\text{packet},t}}}- \frac{v_{\text{delay},t}}{{v_{\text{packet},t}}} \right)\right)\\ 
&+ \beta H\left( \pi\left( l_\text{states}[i]; \theta_a \right)\right).
\end{align*}

$\beta$ was set to $\frac 1 2 10^{-3}$ and $\delta$ to $10^{-3}$.

Reinforcement learning assumes that a reward follows an action and vice-versa (see \autoref{fig:reinforcement}). However, in the case of congestion control, it is desirable to perform a new action without having received a reward for the previous action. For example, imagine that you receive two acknowledgements directly after each other. For each of these two acknowledgements an action has to be performed but it does not seem feasible for the second action to wait until the first action has received a reward. The Asynchronous Actor Critic framework as described by \cite{mnih_asynchronous_2016} cannot be applied to congestion control as it assumes that actions and rewards are synchronized.

\begin{figure}
\includesvg{figures/Reinforcement_learning_diagram}
\caption{The classic reinforcement learning approach.\protect\footnotemark}
\label{fig:reinforcement}
\end{figure}
\footnotetext{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}

Thus we propose the Partial Action Actor Critic Learning (PAL) framework (see \autoref{fig:pal}) in which one action generates a number of partial actions ($leq 1$). Each partial interaction generates a partial reward upon generating with the environment. Upon receiving a partial reward, the agent estimates the current state and triggers a new action. When all partial rewards of one action were received, the agent combines them to form the reward and updates the value and actor networks (see \autoref{alg:pal}). 

\begin{figure}
\includesvg{figures/Reinforcement_learning_diagram_PAL}
\caption{Partial Action Actor Critic Learning: An action consists of zero or more partial actions which trigger partial rewards upon interacting with the environment. Each partial reward updates the state. The value and actor networks are updated upon receiving all partial rewards of one action.\protect\footnotemark}
\label{fig:pal}
\end{figure}
\footnotetext{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}

\begin{algorithm}
\caption{Partial Action Actor Critic Learning -- pseudocode for each learning thread.}
\label{alg:pal}
\begin{algorithmic}[1]
\Loop
	\State $l_\text{actions} \gets  [] $
	\State $l_\text{states} \gets  [] $
	\State $l_\text{values} \gets  [] $
	\State $l_\text{rewards} \gets  [] $
	\State $l_\text{estimatedValues} \gets  [] $
	\State $l_\text{snapshots} \gets  [] $
	\State $t \gets 0$
	\State $s_0 \gets \text{initialState}()$
	\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
	\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
	\Repeat
		\State $l_\text{states}.\text{append}(s_t)$
		\State $l_\text{estimatedValues}.\text{append}(V(s_t; \theta_\text{v}))$
		\If{$t \bmod t_\text{max} = 0$}
			\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
			\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
			\State $l_\text{snapshots}.\text{append}((\theta_\text{a}, \theta_\text{v}))$
		\EndIf
		\State Sample $a_t$ from the probability distribution
		\State $l_\text{actions}.\text{append}(a_t)$
		\State $l_{\text{partialActions},t} \gets \text{partialActions}(a_t)$
		\ForAll{partial actions $a_{{\text{p},n,t}}$ in $l_{\text{partialActions},t}$}
			\State Take partial action $a_{{\text{p},n,t}}$
		\EndFor
		\State Receive partial reward $r_{\text{p},n,t'}$
		\StateIndent[3] where $t' \leq t$ and $0 \leq n \leq\#(l_{\text{partialActions},t'})$
		\If{all partial rewards of $a_{t'}$ were received}
			\State \Call{computeGradients}{}
		\EndIf
		\State Generate $s_{t+1}$ using $r_{\text{p}_{t'}}$
		\State $t \gets t+1$
	\Until{reaching the end the episode}
\EndLoop
%\algstore{palalg}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Partial Action Actor Critic Learning -- procedure which computes and applies the gradients.}
\label{alg:grad}
\begin{algorithmic}[1]%
%\algrestore{palalg}
\Function{computeGradients}{}
	\State $r_{t'} \gets \text{combination of all partial rewards }r_{\text{p},n,t'}$
	\State $l_\text{rewards}.\text{append}(r_{t'})$
	\If{$\#\left(l_\text{rewards}\right) > t_\text{max}$ or the episode is over}
		\State $t_\text{end} = \min\left(t_\text{max}, \#\left( l_\text{rewards} \right)\right)$
		\State $\theta_\text{backup} \gets \left(\theta_\text{a}, \theta_\text{v}\right)$
		\State $\theta_\text{a}, \theta_\text{v} \gets l_\text{snapshots}[0]$
		\State $R_{i+1} \gets \frac{l_\text{estimatedValues}[t_\text{end}]}{1-\gamma}$
		\For{$i\gets t_\text{end}-1, 0$}
			\State $R_i \gets r_i + \gamma r_{i+1}$
			\State $a \gets l_\text{actions}[i]$
			\State $s \gets l_\text{states}[i]$
			\State $v \gets l_\text{values}[i]$
			\State $d\theta_\text{v} \gets \frac{\partial\left(R_i - l_\text{values}[i]\right)}{\partial\theta_v}$
			\State $d\theta_\text{v} \gets -\frac{\partial\log \left( \pi_W\left( a-1 < W \leq a \given s; \theta_\text{a} \right)\right)\left(R_i - v\right)}{\partial\theta_a}$
			\StateIndent[6] $-\frac{\partial\beta H\left( \pi\left( l_\text{states}[i]; \theta_a \right)\right)}{\partial\theta_a}$
			\State $\theta_{\text{a},\text{g}} \gets \theta_{\text{a},\text{g}} + d\theta_\text{a}$
			\State $\theta_{\text{v},\text{g}} \gets \theta_{\text{v},\text{g}} + d\theta_\text{v}$
		\EndFor
		\State $l_\text{actions} \gets l_\text{actions}[t_\text{max}:] $
		\State $l_\text{states} \gets l_\text{states}[t_\text{max}:] $
		\State $l_\text{values} \gets l_\text{values}[t_\text{max}:] $
		\State $l_\text{rewards} \gets l_\text{rewards}[t_\text{max}:] $
		\State $l_\text{estimatedValues} \gets l_\text{estimatedValues}[t_\text{max}:] $
		\State $l_\text{snapshots} \gets l_\text{snapshots}[1:] $
		\State $\theta_\text{a}, \theta_\text{v} = \theta_\text{backup}$
	\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
 
\bibliographystyle{ACM-Reference-Format}
\bibliography{congestion}

\end{document}
