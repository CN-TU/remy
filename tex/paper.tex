\documentclass[sigconf]{acmart}

\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For formal tables
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}

%\usepackage{todonotes}
\usepackage{svg}

\def\xcolorversion{2.00}
\def\xkeyvalversion{1.8}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}

\newcommand\note[2]{{\color{#1}#2}}
\newcommand\todo[1]{{\note{red}{TODO: #1}}}

\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} (\nameref*{#1})}} % One single link

% Adds a "given" symbol (vertical bar) that rescales in height (like \left and \right)
\usepackage{mathtools}
\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase
\newcommand\sgiven{\givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\given\sgiven #1}
\newcommand\Average{E\Basics}

\makeatletter
\newcommand{\StateIndent}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \Statex\hskip\dimexpr#1\@tempdima\relax}
\algdef{S}[WHILE]{WhileNoDo}[1]{\algorithmicwhile\ #1}%
\makeatother

\newcommand{\algorithmautorefname}{Algorithm}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA} 
\acmYear{2017}
\copyrightyear{2017}

\acmPrice{15.00}


\begin{document}
\title{Let's Play Congestion Control}
\titlenote{Produces the permission block, and
  copyright information}
\subtitle{Extended Abstract}
\subtitlenote{The full version of the author's guide is available as
  \texttt{acmart.pdf} document}

\author{Maximilian Bachl} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{maximilian.bachl@tuwien.ac.at}

\author{Tanja Zseby} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{tanja.zseby@tuwien.ac.at}

\author{Joachim Fabini} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{joachim.fabini@tuwien.ac.at}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Bachl, Zseby, Fabini}

\begin{abstract}
This paper provides a sample of a \LaTeX\ document which conforms,
somewhat loosely, to the formatting guidelines for
ACM SIG Proceedings.\footnote{This is an abstract footnote}
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Put some comma-separated keywords here}


\maketitle

\section{Partial Action Actor Critic Learning}
\label{sec:pal}

Partial Action Actor Critic Learning (PAL) is a modification of the Asynchronous Advantage Actor Critic algorithm proposed by \citet{mnih_asynchronous_2016}. It consists of two Artificial Neural Networks (ANNs), the Actor Network and the Value Network (the Critic part in the abbreviation stands for the Value Network). Given a state, the Actor Networks outputs what it deems to be the optimum action to perform in that certain state. The Value Network estimates what long-term reward can be expected in this state. So an action is considered good if it achieved a long-term reward that is higher than the long-term reward expected by the Value Network and it is considered bad if the reward was lower than expected. The long-term reward is implemented as a moving average of future rewards. So if a high reward can be achieved right now this is more favorable than if it can be achieved in the future. However it can also be beneficial to get a low reward now and instead get a very large one in the future. 

\subsection{Algorithm}
\label{subsec:alg}

In \autoref{alg:pal} we show the code that runs in one of the learning threads. As proposed in literature, we use several concurrent threads to improve training performance. A major difference to previous approaches to reinforcement learning is the fact that we allow more actions to be taken before the previous actions' rewards have been received. In classical reinforcement learning, an action is always followed by a reward and a reward is always followed by a action. In our proposed concept, however, it is possible to take new actions while the previous action hasn't received its reward yet.

\begin{figure}
\begin{minipage}{\columnwidth}
\includesvg{figures/Reinforcement_learning_diagram}
\caption{The classical reinforcement learning approach.\protect\footnote{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}}
\label{fig:reinforcement}
\end{minipage}
\end{figure}

Another major difference in PAL is that one action generates a number of partial actions ($\geq 0$) (see \autoref{fig:pal}). Each partial action generates feedback upon interacting with the environment. Upon receiving feedback for a partial action, the agent determines the current state and triggers a new action. When all partial rewards of one action were received, the agent combines them to form the reward and updates the value and actor networks (see \autoref{alg:grad}).

\begin{figure}
\begin{minipage}{\columnwidth}
\includesvg{figures/Reinforcement_learning_diagram_PAL}
\caption{Partial Action Actor Critic Learning: An action consists of zero or more partial actions which trigger partial rewards upon interacting with the environment. Each partial reward updates the state. The value and actor networks are updated upon receiving all partial rewards of one action.\protect\footnote{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}}
\label{fig:pal}
\end{minipage}
\end{figure}

\begin{algorithm}
\caption{Partial Action Actor Critic Learning -- pseudocode for each learning thread. Only $\theta_{\text{a},\text{g}}$ and $\theta_{\text{v},\text{g}}$ are shared between the threads. They are initialized randomly in the beginning. \todo{Update algorithm}}
\label{alg:pal}
\begin{algorithmic}[1]
\Loop
	\State $l_\text{actions} \gets  [] $
	\State $l_\text{states} \gets  [] $
	\State $l_\text{values} \gets  [] $
	\State $l_\text{rewards} \gets  [] $
	\State $l_\text{estimatedValues} \gets  [] $
	\State $l_\text{snapshots} \gets  [] $
	\State $t \gets 0$
	\State $s_0 \gets \text{initialState}()$
	\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
	\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
	\Repeat
		\State $l_\text{states}.\text{append}(s_t)$
		\State $l_\text{estimatedValues}.\text{append}(V(s_t; \theta_\text{v}))$
		\If{$t \bmod t_\text{max} = 0$}
			\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
			\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
			\State $l_\text{snapshots}.\text{append}((\theta_\text{a}, \theta_\text{v}))$
		\EndIf
		\State Sample $a_t$ from the 
		\StateIndent[3] Actor Network's probability distribution
		\State $l_\text{actions}.\text{append}(a_t)$
		\State $l_{\text{partialActions},t} \gets \text{partialActions}(a_t)$
		\ForAll{partial actions $a_{{\text{p},n,t}}$ in $l_{\text{partialActions},t}$}
			\State Take partial action $a_{{\text{p},n,t}}$
		\EndFor
		\State Receive feedback $r_{\text{p},n,t'}$
		\StateIndent[3] where $t' \leq t$ and $0 \leq n \leq\#(l_{\text{partialActions},t'})$
		\If{all feedback of $a_{t'}$ was received}
			\State $r_{t'} \gets \text{reward w.r.t all feedback }r_{\text{p},n,t'}$
			\State $l_\text{rewards}.\text{append}(r_{t'})$
			\If{$\#\left(l_\text{rewards}\right) > t_\text{max}$ or the episode is over}
				\State \Call{computeGradients}{}
			\EndIf
		\EndIf
		\State Generate $s_{t+1}$ using $r_{\text{p}_{t'}}$
		\State $t \gets t+1$
	\Until{reaching the end the episode}
\EndLoop
%\algstore{palalg}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Partial Action Actor Critic Learning -- procedure which computes and applies the gradients.}
\label{alg:grad}
\begin{algorithmic}[1]%
%\algrestore{palalg}
\Function{computeGradients}{}
	\State $t_\text{end} = \min\left(t_\text{max}, \#\left( l_\text{rewards} \right)\right)$
	\State $\theta_\text{backup} \gets \left(\theta_\text{a}, \theta_\text{v}\right)$
	\State $\theta_\text{a}, \theta_\text{v} \gets l_\text{snapshots}[0]$
	\State $R_{i+1} \gets \frac{l_\text{estimatedValues}[t_\text{end}]}{1-\gamma}$
	\For{$i\gets t_\text{end}-1, 0$}
		\State $R_i \gets r_i + \gamma R_{i+1}$
		\State $a \gets l_\text{actions}[i]$
		\State $s \gets l_\text{states}[i]$
		\State $v \gets l_\text{values}[i]$
		\State $d\theta_\text{v} \gets \frac{\partial\left(R_i - v\right)}{\partial\theta_v}$
		\State $d\theta_\text{v} \gets -\frac{\partial\log \left( \pi\left( a \given s; \theta_\text{a} \right)\right)\left(R_i - v\right)}{\partial\theta_a}$
		\StateIndent[4] $-\frac{\partial\beta H\left( \pi\left( s; \theta_a \right)\right)}{\partial\theta_a}$
		\State $\theta_{\text{a},\text{g}} \gets \theta_{\text{a},\text{g}} + d\theta_\text{a}$
		\State $\theta_{\text{v},\text{g}} \gets \theta_{\text{v},\text{g}} + d\theta_\text{v}$
	\EndFor
	\State Remove first $t_\text{end}$ elements from
	\StateIndent[2] $l_\text{actions}$, $l_\text{states}$, $l_\text{values}$, $l_\text{rewards}$, $l_\text{estimatedValues}$
	\State Remove first element from $l_\text{snapshots}$
	\State $\theta_\text{a}, \theta_\text{v} = \theta_\text{backup}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Value Network}
\label{subsec:genericvalue}

The value network estimates the expected long-term reward given a state $s_t$.

Let $r_t$ be the reward that was received at time $t$. $V(s_t; \theta_\text{v})$ designates the expected mean reward in state $s_t$ given the parameters (neural network weights) of the value network $\theta_v$. Then, with $\gamma$ being the roll-off factor, which designates the influence that future reward has on the moving average, (set to $0.99$), we define the average reward as 
\begin{align*}
R_t = \left(\left(\sum_{i=0}^{k-1} \gamma^ir_{t+i}\right) + \gamma^k V(s_{t+k}; \theta_\text{v})\right),
\end{align*}
where $k$ is upper-bounded by $t_\text{max}$ ($t_\text{max}$ is a fixed hyperparameter that indicates how many steps should be performed before updating the neural network). So $R$ is simply a moving average of rewards. However, usual moving averages take into account values from the past while this one uses values from the future; it runs reversely. 

The loss function, which the value network tries to minimize, is the square of the difference of the actual average reward received and the expected average reward
\begin{align*}
l_{\text{v},t} = \left(R_t - V(s_t; \theta_\text{v})\right)^2.
\end{align*}

\subsection{Actor Network}
\label{subsec:genericactor}

Let $a_t$ be the action that was taken at time step $t$, $v_t$ the value that the value network estimated as the future reward given the current state $s_t$ at time step $t$ ($v_t$ is just an abbreviation for $V(s_t; \theta_\text{v})$) and $\theta_a$ the parameters (neural network weights) of the actor network. Furthermore, let $\beta$ be a factor that specifies the importance of the entropy $H$. $\pi$ designates the probability density function, which means that $\pi\left( a_t \given s_t; \theta_\text{a} \right)$ is the value of the probability density function of taking action $a_t$ in state $s_t$ with the current weights of the actor network $\theta_\text{a}$. Then
\begin{align*}
l_{\text{a},t} =& -\log \left( \pi\left( a_t \given s_t; \theta_\text{a} \right)\right)\left( R_t - v_t \right)\\ 
&- \beta H\left( \pi\left( s_t; \theta_a \right)\right)
\end{align*}
is the actor loss that we try to minimize. 

\section{Congestion Control specifics}

\begin{figure}

\newcommand{\nodedistance}{2}
\newcommand{\nd}[1]{\numexpr #1 * \nodedistance \relax}

\begin{tikzpicture}[node distance=\nodedistance,>=stealth',bend angle=45,auto]

  \tikzstyle{place}=[circle,thick,draw=blue!75,fill=blue!20,minimum size=6mm]
  \tikzstyle{red place}=[place,draw=red!75,fill=red!20]
  \tikzstyle{transition}=[rectangle,thick,draw=black!75,
  			  fill=black!20,minimum size=4mm]

%  \tikzstyle{every label}=[blue!75]

  \begin{scope}
    \node [place,tokens=1] (s1) at (\nd{1}, \nd{3}) [label=Received feedback] {};
    \node [place] (s2) at (\nd{1}, \nd{2}) [label={[align=center]left:Received\\ACKs}] {};
    \node [place] (s3) at (\nd{3}, \nd{2}) [label={[align=center]above:Packets\\in the\\network}] {};
    \node [place,tokens=1] (s4) at (\nd{1}, \nd{0}) [label={[align=center]below:Open\\congestion\\window\\(cwnd)}] {};      

    \node [transition] (e1) at (\nd{0}, \nd{1}) [label=left:Take action] {}
      edge [post,bend right] node {$a$}(s4)
      edge [pre,bend left] node {1} (s1);    
     
    \node [transition] (e2) at (\nd{2}, \nd{1}) [label=Send packet] {}
      edge [pre,bend left] node {1} (s4)
      edge [post,bend right] node {1} (s3);

    \node [transition] (e3) at (\nd{2}, \nd{2}) [label=below:Receive ACK] {}
      edge [pre] node {1} (s3)
      edge [post] node {1} (s2);

    \node [transition] (e4) at (\nd{1}, \nd{1}) [label={[align=center]left:Update\\open\\cwnd}] {}
      edge [pre] node {1} (s2)
      edge [post] node {1} (s4);
      
    \node [transition] (e5) at (\nd{2}, \nd{3}) [label={[align=center]right:Receive feedback}] {}
      edge [pre, bend left] node {1} (s2)
      edge [post] node {1} (s1);

  \end{scope}

\end{tikzpicture}

\caption{A petri net describing the congestion control mechanism. We start with one token in the state \textit{Open congestion window}, which means that initially we can send one packet. We start with another token in the state \textit{Received feedback}, which means that we can take an action right in the beginning. The concept is the following: If there is at least one token in the \textit{Open congestion window}, send a packet. The acknowledgement (or timeout if the packet gets lost) for the packet is received by the \textit{Open congestion window}, meaning that when we get an ACK we can send another packet (The \textit{Open congestion window} signifies the congestion window minus the packets that are currently unacknowledged.). Furthermore the state \textit{Received feedback} receives each ACK/timeout and takes an action. \textit{Take action} adds \textit{a} tokens to the \textit{Open congestion window}, where $a$ is a real number (possibly also negative); thus in each state there can also be a real number of tokens (e.g.~ 2.34 tokens are possible in the \textit{Open congestion window}). However, we define that the congestion window can never be smaller than 1. \todo{Draw as 3 times 3 square}}
\label{fig:petri}
\end{figure}

The classical Reinforcement Learning assumes that a reward follows an action and vice-versa (see \autoref{fig:reinforcement}). However, in the case of congestion control, it is desirable to perform a new action without having received a reward for the previous action. For example, imagine that you receive two acknowledgements directly after each other. For each of these two acknowledgements an action has to be performed but it does not seem feasible for the second action to wait until the first action has received a reward. The Asynchronous Actor Critic framework as described by \cite{mnih_asynchronous_2016} cannot be applied to congestion control as it assumes that actions and rewards are synchronized and thus we use the proposed Partial Action Actor Critic Learning (see \autoref{sec:pal}). We describe the concept of using reinforcement learning for congestion control with a petri net (see \autoref{fig:petri}).

In the following a time step $t$ corresponds to the reception of an acknowledgement. The beginning of the flow, before any packet is sent, corresponds to time step $0$.

\begin{description}
\item[$\textit{s}_\text{t}$] The state  describes the current ``congestion state''. Various features that indicate congestion are included in it. Each time an acknowledgement is received, the state is updated and the actor network is asked for the next action. 
\item[$\textit{a}_\text{t}$] Based on a given state and the history of previous states, the actor network returns an action $a_t$, which is a real number ($\geq 1$) that stands for the congestion window to be used until the next acknowledgement is received. 
\item[$\textit{r}_\text{t}$] The reward is a tuple of at least one reward metric. These are discussed in more detail in \autoref{subsec:value}
\item[$\textit{v}_\text{t}$] The value is a tuple of the expected average reward estimated by the value network (see \autoref{subsec:value}) for each of the reward metrics. 
\end{description}

We actually use four different types of reward: 
\begin{description}
\item[$\textit{r}_{\text{packet},t}$] is the number of the packets that the sender sent during time step $t$ and that were not lost (so they were acknowledged at some point by the receiver).
\item[$\textit{r}_{\text{byte},t}$] is the sum of the bytes of the packets that the sender sent and that were not lost. Example: During $t$, three packets were sent with 300, 200, and 1500 bytes each so $\textit{r}_{\text{byte},t} = 2000$
\item[$\textit{r}_{\text{delay},t}$] is the sum of the round trip times of the packets that the sender sent and that were not lost.
\item[$\textit{r}_{\text{duration},t}$] is the sum of the time between receiving the last packet and receiving this packet (``inter-receive time'') for the packets that the sender sent and that were not lost.
\end{description}

%We actually use four moving averages for four different types of reward: 
%\begin{description}
%\item[$\textit{R}_\text{packet}$] is the moving average of the packets that the sender sent and that were not lost (so they were acknowledged at some point by the receiver).
%\item[$\textit{R}_\text{byte}$] is the moving average of the bytes of the packets that the sender sent and that were not lost.
%\item[$\textit{R}_\text{delay}$] is the moving average of the round trip time of the packets that the sender sent and that were not lost.
%\item[$\textit{R}_\text{duration}$] is the moving average of the time between receiving the last packet and receiving this packet (``inter-receive time'') for the packets that the sender sent and that were not lost.
%\end{description}

\subsection{Value Network}
\label{subsec:value}

In the case of congestion control, the loss function $l_{\text{v},t}$ of the value network is actually the sum of the square of the difference for each of the four moving averages for each type of reward:
\begin{align*}
l_{\text{v},t} =& \left(R_{\text{packet},t} - V_\text{packet}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{byte},t} - V_\text{byte}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{delay},t} - V_\text{delay}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{duration},t} - V_\text{duration}(s_t; \theta_\text{v})\right)^2
\end{align*}

\subsection{Actor Network}
\label{subsec:actor}

The actor network outputs two parameters: The mean of a normal distribution $\mu$ and its standard deviation $\sigma$. 

Each time an action $a_t$ is requested, a value $X$ is sampled from the current normal distribution defined by the parameters $\mu$ and $\sigma$: 
\begin{align*}
X&\sim {\mathcal {N}}(\mu ,\sigma^{2})\\
\end{align*}

$a_t$ is the sampled value $X$ at time $t$ and the window is incremented by $a_t$; however, the window can never be smaller than 1. At the beginning of a flow the window is 1 as well.

With $H$ being the entropy, the actor network minimizes the loss
\begin{align*}
l_{\text{a},t} =& -\log \left( \pi \left( a_t \given s_t ; \theta_\text{a} \right) \right)\\
&\left( \left(\log\left(\frac{R_{\text{byte},t}}{{R_{\text{duration},t}}}\right) - \log\left(\frac{v_{\text{byte},t}}{{v_{\text{duration},t}}}\right)\right) - \left( \frac{R_{\text{delay},t}}{{R_{\text{packet},t}}}- \frac{v_{\text{delay},t}}{{v_{\text{packet},t}}} \right)\right)\\ 
&+ \beta H\left( \pi\left( s_t; \theta_a \right)\right).
\end{align*}
 
\subsection{Example}

\begin{enumerate}
\item Receive an ACK
\item Update the internal state to reflect the information that this ACK provides (for example, the state consists of the round trip time experienced by this packet, the time since we receive the last ACK etc.)
\item Sample an increase for the congestion window from the Actor Network (e.g. $0.24$) and add it to the current congestion window.
\item 
\begin{enumerate}
\item If this was the last ACK, which completes a previous action (e.g. a previous action resulted in $3$ packets being sent; so if this is the ACK for the third (and last) packet of this previous action we got all the ACKs to compute the reward.)
\item If we now have $t_{max}$ rewards (set to 20) then we also update the neural network. 
\end{enumerate}

\end{enumerate}

\subsection{Convergence}

We use 

\begin{align*}
\text{Reward} = \log(\text{throughput}) - \delta\text{delay}
\end{align*}

as our metric of reward, where $\delta$ is a parameter that allows us to change the importance of the throughput vs.~the delay. We take the logarithm of the throughput to ensure that flows which have small throughput have a higher incentive to increase it as flows which already have a large one (because the derivative of the logarithm decreases with increasing value). We take minus the delay to punish inducing delay. We do not take the logarithm because we think that it is equally bad if a sender behind a satellite connection with a minimum RTT of $500\,$ms adds $1\,$ms of delay as if a sender connected using Ethernet with a minimum RTT of $10\,$ms adds $1\,$ms of delay. 

There are two conditions that we have to take into account to ensure convergence:
\begin{enumerate}
\item \begin{align*}
\left(\frac{\partial}{\partial w}\text{throughput}\right)\left(1\right) > \left(\frac{\partial}{\partial w}\delta\text{delay}\right)\left(1\right)
\end{align*}
\item \begin{align*}
\frac{\partial}{\partial w}\left(\delta\right)
\end{align*}
\end{enumerate}

\todo{Finish second constraint for convergence.}

\todo{Add figure to visually explain that}
 
\section{Results}

We train on the following scenario:
 
\begin{table}[h]
\centering
\begin{tabular}{rll}
\toprule
Parameter & Value & Distribution \\
\midrule
Two-way propagation delay & $100\,$ms & constant \\
Bottleneck bandwidth & $3\,$Mbit/s & constant \\
Number of senders & $2$ & constant \\
Flow length & $\infty$ & constant \\
Simulation duration & $50\,$s--$150\,$s & uniform \\
Buffer size & $\infty$ & constant \\
Stochastic loss prob. & $0\%$ & constant \\
\bottomrule
\end{tabular}
\caption{The parameters of our very first simple experiment}
\label{tab:smallexperiment}
\end{table}

\todo{Insert results when comparing to regular TCP}
 
\bibliographystyle{ACM-Reference-Format}
\bibliography{congestion}

\end{document}
