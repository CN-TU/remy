\documentclass[sigconf]{acmart}

\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For formal tables
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[inline]{enumitem}

%\usepackage{todonotes}
\usepackage{graphicx}
\graphicspath{figures}
\usepackage{svg}

\def\xcolorversion{2.00}
\def\xkeyvalversion{1.8}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}

\newcommand\note[2]{{\color{#1}#2}}
\newcommand\todo[1]{{\note{red}{TODO: #1}}}

\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} (\nameref*{#1})}} % One single link

% Adds a "given" symbol (vertical bar) that rescales in height (like \left and \right)
\usepackage{mathtools}
\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase
\newcommand\sgiven{\givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\given\sgiven #1}
\newcommand\Average{E\Basics}

\makeatletter
\newcommand{\StateIndent}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \Statex\hskip\dimexpr#1\@tempdima\relax}
\algdef{S}[WHILE]{WhileNoDo}[1]{\algorithmicwhile\ #1}%
\makeatother

\newcommand{\algorithmautorefname}{Algorithm}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA} 
\acmYear{2017}
\copyrightyear{2017}

\acmPrice{15.00}


\begin{document}
\title{Some title}
\titlenote{Produces the permission block, and
  copyright information}
\subtitle{Extended Abstract}
\subtitlenote{The full version of the author's guide is available as
  \texttt{acmart.pdf} document}

\author{Maximilian Bachl} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{maximilian.bachl@tuwien.ac.at}

\author{Tanja Zseby} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{tanja.zseby@tuwien.ac.at}

\author{Joachim Fabini} 
\affiliation{%
 \institution{Institute of Telecommunications}
 \streetaddress{Gußhausstraße 25}
 \postcode{1040}
 \city{Vienna} 
 \country{Austria}}
\email{joachim.fabini@tuwien.ac.at}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Bachl, Zseby, Fabini}

\begin{abstract}
This paper proposes a new method of congestion control in packet-switched networks that uses on-line reinforcement learning to determine the optimum congestion window size at a given point in time with respect to a chosen utility function. 

To this end we use an Artificial Neural Network based approach that can be initialized with a previously trained neural network or with random weights (without any preknowledge). When being used without any preknowledge the approach converges to a stable solution within the order of minutes and then outperforms existing congestion control algorithms with respect to throughput. 

As in case of congestion control feedback used for reinforcement learning is naturally always delayed (due to the round trip time), we propose a novel approach to reinforcement learning that can handle delayed rewards as well as rewards that are returned in parts one after another. 

To our knowledge (\todo{Make sure this is really true!})

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Put some comma-separated keywords here}

\settopmatter{printfolios=true}

\maketitle%

\section{Introduction}

In recent years, there have been several approaches to make congestion control learnable, most notably \textit{Remy} \citep{winstein_tcp_2013}, which learns an optimum congestion control w.r.t.~a specific utility function for specified network scenarios in an off-line fashion. Furthermore there is \textit{Performance-oriented Congestion Control} (PCC) \citep{dong_pcc:_2015}, which repeatedly tries different rates during periods of approximately one round-trip time (RTT) and chooses the better rate w.r.t.~a specific utility function. 

We make use of a new Reinforcement Learning (RL) based approach that can learn a congestion control algorithm on-line and is able to perform actions on a per acknowledgement basis. 

\subsection{Actor Critic Learning}
\label{subsec:ac}

To this end we propose \textit{Partial Action Learning} (PAL) (see \ref{subsec:pal}) which is a modification of the Actor Critic framework for neural networks proposed by \citet{mnih_asynchronous_2016}, which we outline in this section. 

There are two Artificial Neural Networks (ANNs), the Actor Network and the Value Network (the Critic part in the abbreviation stands for the Value Network). Given a state, the Actor Network outputs what it deems to be the optimum action to perform in that certain state. The Value Network estimates what long-term reward can be expected in this state. So an action is considered good if it achieved a long-term reward that is higher than the long-term reward expected by the Value Network and it is considered bad if the reward was lower than expected. The long-term reward is implemented as a moving average of future rewards. So if a high reward can be achieved right now this is more favorable than if it can be achieved in the future. However it can also be beneficial to get a low reward now and instead get a very large one in the future. 

\subsubsection{Value Network}
\label{subsubsec:genericvalue}

The Value Network outputs the expected long-term reward $V(s_{t}; \theta_\text{v})$ given a state $s_t$ at time step $t$ and the parameters (neural network weights) of the value network $\theta_v$.

With $r_t$ being the reward that was received at time $t$ and $\gamma$ being the roll-off factor, which stands for the influence that future reward has on the moving average (commonly set to $0.99$), we define the expected long-term reward as 
\begin{align*}
R_t = \left(\left(\sum_{i=0}^{k-1} \gamma^ir_{t+i}\right) + \gamma^k V(s_{t+k}; \theta_\text{v})\right)\left( 1-\gamma \right),
\end{align*}
where $k$ is upper-bounded by $t_\text{max}$ ($t_\text{max}$ is a fixed hyperparameter that indicates how many rewards should be received before updating the neural network). So $R_t$ is simply a moving average of rewards at time step $t$. However, usual moving averages take into account values from the past while this one uses values from the future. In practice this means that we collect $t_\text{max}$ rewards, compute the expected long-term reward starting at each time step, update the neural network and start the same procedure again. 

%\todo{Explanation not so clear?}

The loss function, which the value network tries to minimize at each time step, is the square of the difference of the actual long-term reward received and the expected long-term reward
\begin{align*}
l_{\text{v},t} = \left(R_t - V(s_t; \theta_\text{v})\right)^2.
\end{align*}

\subsubsection{Actor Network}
\label{subsubsec:genericactor}

The Actor Network outputs a probability distribution from which the action $a_t$ at time step $t$ is randomly sampled. We use the mean $\mu$ and the standard deviation $\sigma$, to parametrize a normal distribution. The main idea is that the network learns to output the right mean at the right time step to maximize the future reward and that it uses the standard deviation to try out new actions, which could yield a better than expected reward. 

With $v_t$ being the value that the value network estimated as the future reward given the current state $s_t$ at time step $t$ ($v_t$ is just an abbreviation for $V(s_t; \theta_\text{v})$) and $\theta_a$ the parameters of the actor network and with $\beta$ being a factor that specifies the importance of the entropy $H$, $\pi$ designating the probability density function, meaning that $\pi\left( a_t \given s_t; \theta_\text{a} \right)$ is the value of the probability density function of taking action $a_t$ in state $s_t$ with the current weights of the actor network $\theta_\text{a}$, we define the loss that the actor network aims to minimize as follows:
\begin{align*}
l_{\text{a},t} =& -\log \left( \pi\left( a_t \given s_t; \theta_\text{a} \right)\right)\left( R_t - v_t \right)\\ 
&- \beta H\left( \pi\left( s_t; \theta_a \right)\right).
\end{align*}

\section{Method}
\subsection{Partial Action Learning}
\label{subsec:pal}

The key difference between PAL and previous approaches to Reinforcement Learning is that in classical Reinforcement Learning, an action is always followed by a reward and a reward is always followed by a action. In our proposed concept, however, it is possible to take new actions while previous actions haven't received their rewards yet.

Another major difference in PAL is that one action generates a number of partial actions ($\geq 0$) (see \autoref{fig:pal}). Each partial action generates feedback upon interacting with the environment. Upon receiving feedback for a partial action, the agent determines the current state and triggers a new action. When all feedbacks of one action were received, the agent combines them to form the reward and updates the value and actor networks.

In \autoref{alg:pal} we show the code that runs in each of the agents (in the congestion control scenario, one agent corresponds to a sender). It is possible to have several agents which share a set of weights but one can also use separate weights for each agent, which is more realistic in case of congestion control in the Internet, as different senders cannot easily share a set of neural network weights over the Internet.

\begin{figure}
\begin{minipage}{\columnwidth}
\includesvg{figures/Reinforcement_learning_diagram}
\caption{The classical Reinforcement Learning approach.\protect\footnote{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}}
\label{fig:reinforcement}
\end{minipage}
\end{figure}

\begin{figure}
\begin{minipage}{\columnwidth}
\includesvg{figures/Reinforcement_learning_diagram_PAL}
\caption{Partial Action Learning: An action consists of zero or more partial actions which trigger feedback upon interacting with the environment. Each feedback updates the state. The value and actor networks are updated upon receiving all feedback of one action.\protect\footnote{adapted from \url{https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg}}}
\label{fig:pal}
\end{minipage}
\end{figure}

\begin{algorithm}
\caption{Partial Action Learning -- pseudocode for each agent. It is possible that the agents share the global weights $\theta_{\text{a},\text{g}}$ and $\theta_{\text{v},\text{g}}$, which can be reasonable when learning off-line. Otherwise an individual copy of them is kept by each agent. All weights are initialized randomly in the beginning. LSTM cells are used in both the actor and the value network.}
\label{alg:pal}
\begin{algorithmic}[1]
\Loop
	\State $l_\text{actions} \gets  [] $
	\State $l_\text{states} \gets  [] $
	\State $l_\text{values} \gets  [] $
	\State $l_\text{rewards} \gets  [] $
	\State $l_\text{estimatedValues} \gets  [] $
	\State $l_\text{snapshots} \gets  [] $
	\State $l_\text{hiddenStates} \gets  [] $
	\State $t \gets 0$
	\State $s_0 \gets \text{initialState}()$
	\State $h_\text{a} \gets \text{initialHiddenState}()$
	\State $h_\text{v} \gets \text{initialHiddenState}()$
	\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
	\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
	\Repeat
		\State $l_\text{estimatedValues}.\text{append}(V(s_t; \theta_\text{v}))$
		\If{$t \bmod t_\text{max} = 0$}
			\State $\theta_\text{a} \gets \theta_{\text{a},\text{g}}$
			\State $\theta_\text{v} \gets \theta_{\text{v},\text{g}}$
			\State $l_\text{snapshots}.\text{append}((\theta_\text{a}, \theta_\text{v}))$
			\State $l_\text{hiddenStates}.\text{append}((h_\text{a}, h_\text{v}))$
		\EndIf
		\State $l_\text{states}.\text{append}(s_t)$
		\State Sample $a_t$ from the 
		\StateIndent[3] Actor Network's probability distribution
		\State $l_\text{actions}.\text{append}(a_t)$
		\State $l_\text{values}.\text{append}(V(s_t; \theta_\text{v}))$
		\State $l_{\text{partialActions},t} \gets \text{partialActions}(a_t)$
		\ForAll{partial actions $a_{{\text{p},i,t}}$ in $l_{\text{partialActions},t}$}
			\State Take partial action $a_{{\text{p},i,t}}$
		\EndFor
		\State Wait for the next feedback $r_{\text{p},j,t'}$
		\StateIndent[3] where $t' \leq t$ and $0 \leq j \leq\#(l_{\text{partialActions},t'})$
		\If{all feedback of $a_{t'}$ was received}
			\State $r_{t'} \gets \text{reward w.r.t all feedback }r_{\text{p},k,t'}$
					\StateIndent[4] where $0 \leq k \leq\#(l_{\text{partialActions},t'})$
			\State $l_\text{rewards}.\text{append}(r_{t'})$
			\If{$\#\left(l_\text{rewards}\right) > t_\text{max}$ or the episode is over}
				\State \Call{computeGradients}{}
			\EndIf
		\EndIf
		\State Generate $s_{t+1}$ using $r_{\text{p}_{t'}}$
		\State $t \gets t+1$
	\Until{reaching the end the episode}
\EndLoop
%\algstore{palalg}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Partial Action Learning -- procedure which computes and applies the gradients.}
\label{alg:grad}
\begin{algorithmic}[1]%
%\algrestore{palalg}
\Function{computeGradients}{}
	\State $t_\text{end} = \min\left(t_\text{max}, \#\left( l_\text{rewards} \right)\right)$
	\State $\theta_\text{backup} \gets \left(\theta_\text{a}, \theta_\text{v}\right)$
	\State $h_\text{backup} \gets \left(h_\text{a}, h_\text{v}\right)$
	\State $\theta_\text{a}, \theta_\text{v} \gets l_\text{snapshots}[0]$
	\State $h_\text{a}, h_\text{v} \gets l_\text{hiddenStates}[0]$
	\State $R_{i+1} \gets$ last element of $l_\text{estimatedValues}$%
	\For{$i\gets t_\text{end}-1, 0$}
		\State $R_i \gets \left(r_i + \gamma R_{i+1}\right)\left(1-\gamma\right)$
		\State $a \gets l_\text{actions}[i]$
		\State $s \gets l_\text{states}[i]$
		\State $v \gets l_\text{values}[i]$
		\State $d\theta_\text{a} \gets -\frac{\partial\log \left( \pi\left( a \given s; \theta_\text{a} \right)\right)\left(R_i - v\right)}{\partial\theta_a}$
		\StateIndent[4] $-\frac{\partial\beta H\left( \pi\left( s; \theta_a \right)\right)}{\partial\theta_a}$
		\State $d\theta_\text{v} \gets \frac{\partial\left(R_i - v\right)}{\partial\theta_v}$
		\State $\theta_{\text{a},\text{g}} \gets \theta_{\text{a},\text{g}} + d\theta_\text{a}$
		\State $\theta_{\text{v},\text{g}} \gets \theta_{\text{v},\text{g}} + d\theta_\text{v}$
	\EndFor
	\State Remove first $t_\text{end}$ elements from
	\StateIndent[2] $l_\text{actions}$, $l_\text{states}$, $l_\text{values}$, $l_\text{rewards}$, $l_\text{estimatedValues}$
	\State Remove the first element from $l_\text{snapshots}$
	\State Remove the first element from $l_\text{hiddenStates}$
	\State $\theta_\text{a}, \theta_\text{v} = \theta_\text{backup}$
	\State $h_\text{a}, h_\text{v} = h_\text{backup}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Congestion Control specifics}

\begin{figure}

\input{petri_net.tex}

\caption{A petri net describing the congestion control mechanism. We start with one token in the state \textit{Received feedback}, which means that we can take an action right in the beginning. The concept is the following: If there is at least one token in the \textit{Open congestion window}, send a packet. The token goes to the network and after some time the acknowledgement (or timeout if the packet gets lost) for the packet is received in the \textit{Received ACK} state. From here one token goes to the \textit{Open congestion window} meaning that when we get an ACK (or timeout) we can send another packet (The \textit{Open congestion window} signifies the congestion window minus the packets that are currently unacknowledged.). Furthermore the state \textit{Received feedback} receives a token upon receiving an ACK/timeout. \textit{Take action} adds \textit{a} tokens to the \textit{Open congestion window}, where $a$ is a real number (possibly also negative); thus in each state there can also be a real number of tokens (e.g.~ 2.34 tokens are possible in the \textit{Open congestion window}). However, we define that the congestion window can never be smaller than 1 (This means that the sum of all tokens in the \textit{Open congestion window.}, \textit{Received ACK} and \textit{Packets in the network} states can never be smaller than 1.}
\label{fig:petri}
\end{figure}

The motivation for PAL is that classical Reinforcement Learning assumes that a reward follows an action and vice-versa (see \autoref{fig:reinforcement}). However, in the case of congestion control, it is desirable to perform a new action without having received a reward for the previous action. For example, imagine that we receive two acknowledgements directly after one other. For each of these two acknowledgements an action has to be performed but it does not seem feasible for the second action to wait until the first action has received a reward as it takes one RTT until an acknowledgement (a reward) is received. Thus the Asynchronous Actor Critic framework as described by \cite{mnih_asynchronous_2016} cannot be applied to congestion control as it assumes that actions and rewards are synchronized and so we have to use Partial Action Learning (see \autoref{subsec:pal}). We describe the overall workings of PAL for congestion control using a petri net (see \autoref{fig:petri}).

To use PAL for congestion control we first have to define the correct semantics for this specific use case and we have to explicitly state how the state, reward etc.~are defined. Furthermore, we have to define how the actor and value network explicitely work in case of congestion control. In the following a time step $t$ corresponds to the reception of an acknowledgement. The beginning of the flow, before any packet is sent, corresponds to time step $0$.

\begin{description}
\item[$\textit{s}_\text{t}$] The state  describes the current ``congestion state''. The following features are included in it:
\begin{itemize}
\item the round-trip time of the last packet
\item the current congestion window
\item the time between the last two packets that were sent
\item the time between the last two packets that were received
\item the number of packets that were lost since the last acknowledgement was received
\end{itemize}
Each time an acknowledgement is received, the state is updated and the actor network is asked for the next action.
\item[$\textit{a}_\text{t}$] Based on a given state and the history of previous states (because we use LSTM cells and thus can also consider previous states), the actor network returns an action $a_t$, which is a real number that stands for the change of the congestion window. 
\item[$\textit{r}_\text{t}$] The reward is a tuple of at least one reward metric. 
%These are discussed in more detail in \autoref{subsubsec:value}. 
For each reward metric there is also an output of the value network that predicts the expected long-term average of this reward metric given the current state. 
%\item[$\textit{v}_\text{t}$] The value is a tuple of the expected average reward estimated by the value network (see \autoref{subsubsec:value}) for each of the reward metrics. 
\end{description}

We actually use the following types of reward: 
\begin{description}
\item[$\textit{r}_{\text{packet},t}$] is the number of the packets that the sender sent during time step $t$ and that were not lost (so they were acknowledged at some point by the receiver).
%\item[$\textit{r}_{\text{byte},t}$] is the sum of the bytes of the packets that the sender sent and that were not lost. Example: At time $t'$, three packets were sent with 300, 200, and 1500 bytes each so $\textit{r}_{\text{byte},t'} = 2000$
\item[$\textit{r}_{\text{delay},t}$] is the sum of the round trip times of the packets that the sender sent and that were not lost.
\item[$\textit{r}_{\text{duration},t}$] is the sum of the time between receiving the last packet and receiving this packet (``inter-receive time'') for the packets that the sender sent and that were not lost.
\item[$\textit{r}_{\text{lost},t}$] is the sum of packets that were lost between receiving the last packet and receiving this packet.
\end{description}

%We actually use four moving averages for four different types of reward: 
%\begin{description}
%\item[$\textit{R}_\text{packet}$] is the moving average of the packets that the sender sent and that were not lost (so they were acknowledged at some point by the receiver).
%\item[$\textit{R}_\text{byte}$] is the moving average of the bytes of the packets that the sender sent and that were not lost.
%\item[$\textit{R}_\text{delay}$] is the moving average of the round trip time of the packets that the sender sent and that were not lost.
%\item[$\textit{R}_\text{duration}$] is the moving average of the time between receiving the last packet and receiving this packet (``inter-receive time'') for the packets that the sender sent and that were not lost.
%\end{description}

The overall structure of the neural network for both the value and the actor network is depicted in \autoref{fig:neural_network}.

\begin{figure}

\input{neural_network.tex}

\caption{An overview of the complete neural network being used. Ocher stands for the input, blue for linear layers, red for LSTM layers, green for linear layers with a softplus activation function and purple for the outputs of the neural network. The numbers in parentheses next to each label stand for the number of width of that layer (i.e.~the number of neurons except for the inputs and outputs).}
\label{fig:neural_network}
\end{figure}

\subsubsection{Value Network}
\label{subsubsec:value}

In the case of congestion control, the loss function $l_{\text{v},t}$ of the value network is actually the sum of the squares of the difference for each of the expected long-term averages and the empirically found averages for each reward metric:
\begin{align*}
l_{\text{v},t} =& \left(R_{\text{packet},t} - V_\text{packet}(s_t; \theta_\text{v})\right)^2 \\
%&+\left(R_{\text{byte},t} - V_\text{byte}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{delay},t} - V_\text{delay}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{duration},t} - V_\text{duration}(s_t; \theta_\text{v})\right)^2 \\
&+\left(R_{\text{lost},t} - V_\text{lost}(s_t; \theta_\text{v})\right)^2
\end{align*}

\subsubsection{Actor Network}
\label{subsubsec:actor}

The actor network outputs two parameters: The mean of a normal distribution $\mu$ and its standard deviation $\sigma$. 

Each time an action $a_t$ is requested, it is sampled from the current normal distribution defined by the parameters $\mu$ and $\sigma$.

Then the window is incremented by $a_t$; however, the window can never be smaller than 1. At the beginning of a flow the window starts with 1 as well.

With $H$ being the entropy, the actor network minimizes the loss
%\begin{align*}
%l_{\text{a},t} =& -\log \left( \pi \left( a_t \given s_t ; \theta_\text{a} \right) \right)\\
%&\left( \left(\log\left(\frac{R_{\text{byte},t}}{{R_{\text{duration},t}}}\right) - \log\left(\frac{v_{\text{byte},t}}{{v_{\text{duration},t}}}\right)\right) - \left( \frac{R_{\text{delay},t}}{{R_{\text{packet},t}}}- \frac{v_{\text{delay},t}}{{v_{\text{packet},t}}} \right)\right)\\ 
%&+ \beta H\left( \pi\left( s_t; \theta_a \right)\right).

\begin{align*}
l_{\text{a},t} =& -\log \left( \pi \left( a_t \given s_t ; \theta_\text{a} \right) \right)\\
& \left( U_{\text{measured},t} - U_{\text{expected},t} \right)\\ 
&- \beta H\left( \pi\left( s_t; \theta_a \right)\right)
\end{align*}
where $U$ can be any utility function defined based on some of the reward metrics. In the above formulation, one considers how much better the actual experienced Utility was compared to the expected one. The Utility is formed from the previously defined expected long-term values. 
 
\subsection{Utility function}

As the utility function one can choose an arbitrary function based on the reward metrics previously defined. A safe choice is to use PCC's \citep{dong_pcc:_2015} reward function as its convergence has been previously proven. It is defined as follows:

\begin{align*}
U_t = r_{\text{received},t}\text{Sigmoid}_\alpha\left(\frac{r_{\text{loss},t}}{r_{\text{sent},t}} - 0.05\right) + r_{\text{loss},t}
\end{align*}
where 
\begin{align*}
\text{Sigmoid}_\alpha(x) = \frac{1}{1+e^{\alpha x}},
\end{align*}
$r_\text{received}$ is the actual throughput (in packets per second), $r_\text{loss}$ is the loss rate (in packets per second) and $r_\text{sent}$ is the sending rate (in packets per second). Thus $\frac{r_\text{loss}}{r_\text{sent}}$ is the ratio of data getting lost compared to those being sent, which is a number between 0 and 1.

Intuitively it means that one takes the actual throughput (packets that get received by the receiver and do not get lost) times the sigmoid function of the loss ratio minus 0.05 plus the loss rate. The sigmoid function essentially acts as a cutoff threshold. As soon as the loss rate rises above 5\%, the Sigmoid function decreases very quickly and thus diminishes the Utility. 

It is also possible to set the cutoff threshold to 0, which means that losing packets is heavily punished; or one can increase the threshold to allow for more loss. 

$r_{\text{received},t}$ can be calculated as $\frac{R_\text{packet},t}{R_\text{duration},t}$, $r_{\text{loss},t}$ can be calculated as $\frac{R_\text{lost},t}{R_\text{duration},t}$ and $r_{\text{sent},t}$ can be calculated as $\frac{{R_\text{packet},t} + {R_\text{lost},t}}{R_\text{duration},t}$.

Actually, we assume equally sized packets for all of the above methodology, however, it would also easily be possible to consider the number of bytes per packet as well: One would have to add an additional reward metric of the average number of bytes per packet and as well add one more output to the value network. 

\subsection{Example}

In this section, an example of the overall procedure is outlined. 

\begin{enumerate}
\item Receive an ACK
\item Update the internal state to reflect the information that this ACK provides (for example, the state consists of the round trip time experienced by this packet, the time since we receive the last ACK etc.)
\item Sample an increase for the congestion window from the Actor Network (e.g. $0.24$) and add it to the current congestion window.
\item 
\begin{enumerate}
\item If this was the last ACK, which completes a previous action (e.g. a previous action resulted in $3$ packets being sent; so if this is the ACK for the third (and last) packet of this previous action, we got all the ACKs to compute the reward), combine all partial rewards to form the actual reward for that action. 
\item If we now have $t_\text{max}$ rewards (set to 20) then we also update the neural network. 
\end{enumerate}

\end{enumerate}

%\subsection{Convergence}
%
%We use 
%
%\begin{align*}
%\text{Reward} = \log(\text{throughput}) - \delta\text{delay}
%\end{align*}
%
%as our metric of reward, where $\delta$ is a parameter that allows us to change the importance of the throughput vs.~the delay. We take the logarithm of the throughput to ensure that flows which have small throughput have a higher incentive to increase it as flows which already have a large one (because the derivative of the logarithm decreases with increasing value). We take minus the delay to punish inducing delay. We do not take the logarithm because we think that it is equally bad if a sender behind a satellite connection with a minimum RTT of $500\,$ms adds $1\,$ms of delay as if a sender connected using Ethernet with a minimum RTT of $10\,$ms adds $1\,$ms of delay. 
%
%There are two conditions that we have to take into account to ensure convergence:
%\begin{enumerate}
%\item \begin{align*}
%\left(\frac{\partial}{\partial w}\text{throughput}\right)\left(1\right) > \left(\frac{\partial}{\partial w}\delta\text{delay}\right)\left(1\right)
%\end{align*}
%\item \begin{align*}
%\frac{\partial}{\partial w}\left(\delta\right)
%\end{align*}
%\end{enumerate}
%
%\todo{Finish second constraint for convergence.}
%
%\todo{Add figure to visually explain that}

%\subsection{Convergence}
%
%Assume we have $N$ senders, each with its specific baseline delay (this encompasses all delays excluding only the queueing delay) $d_i$. Each sender keeps a congestion window $b_i$. The link speed at the bottleneck is designated as $c$. 
%
%%We define the excessive window as 
%%\begin{align*}
%%b_\text{excess} = \max\left( \left( \sum_{j=1}^N \frac{b_j}{d_j} \right) - c, 0 \right)
%%\end{align*}
%%and the queueing delay as 
%%\begin{align*}
%%d_q = \frac{b_\text{excess}}{c}.
%%\end{align*}
%
%With $b = (b_i)$ we observe the following relation:
%
%\begin{align*}
%c = \sum_{j=1}^N \frac{b_j}{d_j + d_q(b)}
%\end{align*}
%
%Then we define the utility function for each sender $i$ as
%\begin{align*}
%V_i(b) \coloneqq \begin{cases}
%\log\left(\frac{b_i}{d_i}\right) - \delta_i d_i & \text{if } \sum_{j=1}^N \frac{b_i}{d_i} \leq c\\
%\log\left(\frac{b_i}{d_i + d_q(b)}\right) - \delta_i (d_i + d_q(b)) & \text{otherwise}
%\end{cases}
%\end{align*}
%
%\begin{align*}
%&\log\left(\frac{b_i}{d_i + d_q}\right) - \delta_i (d_i + d_q) \\
%=& \log\left(b_i\right) - \log\left(d_i + d_q\right) - \delta_i (d_i + d_q)
%\end{align*}
%
%\begin{align*}
%\frac{\partial V_i}{\partial b_i} = \begin{cases}
%\frac{1}{b_i} & \text{if } \sum_{j=1}^N \frac{b_i}{d_i} \leq c\\
%\frac{1}{b_i} - \frac{\partial d_q}{\partial b_i}\left(b_i\right)\frac{1}{d_i+d_q\left(b_i\right)} - \delta\frac{\partial d_q}{b_i}\left(b_i\right) & \text{otherwise}
%\end{cases}
%\end{align*}

%\begin{align*}
%G(b) = \begin{pmatrix}
%-\frac{1}{b_1^2} & & \\
%& \ddots & \\
%& & -\frac{1}{b_N^2}
%\end{pmatrix} \text{if } \text{if } \sum_{j=1}^N \frac{b_i}{d_i} \leq c
%\end{align*}

%To show diagonal strict concavity we have to show that d.s.c.~is fulfilled \begin{enumerate*} \item for two distinct points lying in $\sum_{j=1}^N \frac{b_i}{d_i} \leq c$ \item for two distinct points lying in $\sum_{j=1}^N \frac{b_i}{d_i} > c$ and \item when one point is in one part of the function and the other in the other one\end{enumerate*}.
%
%\todo{Insert what d.s.c~actually is and quote Altman's paper}

%\begin{enumerate}
%\item (b^1 - 
%\item
%\item
%\end{enumerate}

%\section{Results}
%
%We train on the following scenario:
% 
%\begin{table}[h]
%\centering
%\begin{tabular}{rll}
%\toprule
%Parameter & Value & Distribution \\
%\midrule
%Two-way propagation delay & $100\,$ms & constant \\
%Bottleneck bandwidth & $3\,$Mbit/s & constant \\
%Number of senders & $2$ & constant \\
%Flow length & $\infty$ & constant \\
%Simulation duration & $50\,$s--$150\,$s & uniform \\
%Buffer size & $\infty$ & constant \\
%Stochastic loss prob. & $0\%$ & constant \\
%\bottomrule
%\end{tabular}
%\caption{The parameters of our very first simple experiment}
%\label{tab:smallexperiment}
%\end{table}
%
%\todo{Insert results when comparing to regular TCP}
 
\bibliographystyle{ACM-Reference-Format}
\bibliography{congestion}

\end{document}
