\documentclass[11pt]{beamer}
\usetheme{Boadilla}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{svg}
\def\xcolorversion{2.00}
\def\xkeyvalversion{1.8}
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}

\usepackage{mathtools}
\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase
\newcommand\sgiven{\givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\given\sgiven #1}
\newcommand\Average{E\Basics}

%\author{Maximilian Bachl}
\title{Congestion Control}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 
\begin{document}

\begin{frame}
\titlepage
\end{frame}

%\begin{frame}
%\tableofcontents
%\end{frame}

\begin{frame}{Congestion Control}
\begin{itemize}
\item Why Congestion Control? Before congestion control was invented: Everyone sent as much as they pleased $\rightarrow$ \textit{Congestion Collapse}.
\item Goal: Estimate available bandwidth. Don't send too much, don't send too little.
\item Method: Keep a \textit{Congestion Window}
\item E.g. Congestion Window of 5 means that we can have up to 5 packets somewhere in the network.
\end{itemize}
\end{frame}

\begin{frame}{How does Congestion Control work nowadays?}
Simplified:
\begin{itemize}
\item Congestion Window 1 in the beginning of a flow.
\item Upon receiving an acknowledgement for a previously sent packet, increase the Congestion Window (cwnd): $$\text{cwnd} = \frac{1}{\text{cwnd}}$$
\item Recently also a bit more sophisticated $\rightarrow$ \textit{CUBIC} etc. 
\item When there is packet loss then we sent too much (buffer of a router on the way overflew) $\rightarrow$ congestion $\rightarrow$ decrease Congestion Window
\item The most common thing to do is $$\text{cwnd} = \frac{\text{cwnd}}{2}$$
\end{itemize}
\end{frame}

\begin{frame}{Problems}
\begin{itemize}
\item Only decrease window on loss $\rightarrow$ That's too late! Decrease before when buffers of routers fill up and latency increases!
\item On wireless connections stochastic packet loss is common $\rightarrow$ TCP thinks it's congestion.
\end{itemize}
\end{frame}

\begin{frame}{Potential Solution}
\begin{itemize}
\item Let's build some machine learning thing!
\item Solutions already exist $\rightarrow$ \textit{TCP ex Machina} by Winstein and Balakrishnan (2013).
\item They simulate networks and learn an optimum congestion control more or less by using a brute force algorithm.
\item Example: Use networks with 1 to 5 senders, RTT from 10 to 100\,ms. Use one set of congestion control rules for 1000 simulations. Then change some parameters and check if it improved (actually they do it in a smarter way)
\item Problem: Training has to be done offline. But it would be nice to have a Congestion Control that learns in real time, online!
\end{itemize}
\end{frame}

\begin{frame}{Reinforcement Learning}
\centering
\includesvg{figures/Reinforcement_learning_diagram}
\end{frame}

\begin{frame}{Problems with RL}
\begin{itemize}
\item Let's say an action is increasing the Congestion Window
\item We can calculate the reward when we get back all the ACKs of packets that were sent during an action
\item This means we can't do anything until receiving each action's reward. But that takes at least one Round Trip Time! Doesn't work.
\item Example: We increase the cwnd by 2 packets. So we can send two packets. To evaluate if this action was good we have to get the ACKs of these two packets, which happens after one RTT!
In the meantime another ACK could have arrived. What do we do? Not defined with RL...
\end{itemize}
\end{frame}

\begin{frame}{Partial Actions}
\centering
\includesvg{figures/Reinforcement_learning_diagram_PAL}
\end{frame}

\begin{frame}{Partial Actions: Example}
\begin{itemize}
\item Let's say the window is 1.9. We increase the window by $0.3$ (action). This allows us to send two packets (partial actions).  
\item We receive the ACK for the first packet (feedback). We update the state and perform a new action.
\item We receive the second ACK. Again, we update the state and perform an action. However, because we got all partial rewards of the previous action, we can calculate the reward and update our agent.
\end{itemize}
\textbf{Key point}: We can update the state without receiving the full reward yet.
\end{frame}

\begin{frame}{Asynchronous Actor Critic}
\begin{itemize}
\item A Deep Learning framework for reinforcement learning. Used for learning how to play video games.
\item Maximize long term reward: \begin{align*}
R_t = \left(\left(\sum_{i=0}^{k-1} \gamma^ir_{t+i}\right) + \gamma^k V(s_{t+k}; \theta_\text{v})\right),
\end{align*}
\item The \textbf{Critic} tries to estimate how much (long-term) reward one can expect considering the current state.
\end{itemize}
\end{frame}

\begin{frame}{Asynchronous Actor Critic -- Actor}
\begin{itemize}
\item The \textbf{Actor} tries to perform an action that is better than what the Critic would expect.
\item It outputs two things:
\begin{itemize}
\item What it thinks is the best action (e.g. increase the window by $0.3$)
\item A standard deviation to experiment a little bit (e.g. $0.45$)
\end{itemize}
\item So the actor outputs a normal distribution from which actions are sampled. Thanks to the standard deviation we experiment and don't get stuck with suboptimal actions. 
\end{itemize}
\end{frame}

\begin{frame}{Preliminary results -- Experiment characteristics}
\centering
\begin{tabular}{rll}
\toprule
Parameter & Value & Distribution \\
\midrule
Two-way RTT when queue is 0 & $150\,$ms & constant \\
Bottleneck bandwidth & $15\,$Mbit/s & constant \\
Number of senders & $8$ & constant \\
Flow length & $100\,$kB & exponential \\
Time between flows & $0.5\,$s & exponential \\
Simulation duration & $500\,$s & constant \\
Buffer size & $100$ packets & constant \\
Stochastic loss prob. & $0\%$ & constant \\
\bottomrule
\end{tabular}
\end{frame}

\begin{frame}{Preliminary results -- Comparison}
Stuff goes here!

\end{frame}

\begin{frame}{Petri net}

\input{petri_net.tex}

\end{frame}

\begin{frame}{Neural Network}

\input{neural_network.tex}

\end{frame}

\begin{frame}{Reward 1}
\begin{align*}
\frac{R_\text{packets}}{R_\text{duration}}\text{Sigmoid}_\alpha\left(\frac{R_\text{lost}}{R_\text{lost} + R_\text{packets}} - 0.05\right) - \frac{R_\text{lost}}{R_\text{duration}} - \frac{R_\text{delay}}{R_\text{packets}}
\end{align*}
\end{frame}

\begin{frame}{Reward 2}
\begin{align*}
R_\text{packets}\text{Sigmoid}_\alpha\left(\frac{R_\text{lost}}{R_\text{lost} + R_\text{packets}} - 0.05\right) - R_\text{lost} - \frac{R_\text{delay}}{R_\text{packets}}
\end{align*}
\end{frame}

\end{document}